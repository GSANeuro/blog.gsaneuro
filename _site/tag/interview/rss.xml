
---
layout: atom
title: nil
---

	<item>
	  <title>From 1D to 2D in the electrophysiology of the human retina - an interview with Mathieu Gauvin</title>
	  <link>//2016/05/26/mathieu-gauvin-ipn-interview/</link>
	  <author></author>
	  <pubDate>2016-05-26T15:01:00-04:00</pubDate>
	  <guid>//2016/05/26/mathieu-gauvin-ipn-interview/</guid>
	  <description><![CDATA[
	     <p>Our new interview is with <a href="https://scholar.google.ca/citations?hl=en&amp;user=rb7UxjQAAAAJ" target="_blank">Mathieu Gauvin</a>, Ph.D. student supervised by <a href="http://www.thechildren.com/departments-and-staff/staff/pierre-lachapelle-phd-director-visual-electrophysiology" target="_blank">Dr. Pierre Lachapelle</a> at the McGill Visual Electrophysiology Laboratory and Clinic. Mathieu works on the development of new methods for studying retinal function to better understand both normal and impaired visual processing. His findings (<a href="http://www.ncbi.nlm.nih.gov/pubmed/26746684" target="_blank">published in the Journal of Vision</a>) show how using some cutting-edge techniques when analyzing the human electroretinogram can uncover distinct markers of different types of retinal impairment. In a Q&amp;A session with <a href="https://blog.gsaneuro.com/editors/">Anastasia Glushko</a>, Mathieu told us about his project and its clinical potential!</p>

<hr />

<p><strong>A (Anastasia): Hi, Mathieu! Thanks again for agreeing to answer our questions. Can you tell us in a couple of sentences what the main goal of your PhD research is?</strong></p>

<p><strong>M (Mathieu):</strong> As you might know, to study retinal function, scientists and clinicians rely on the electroretinogram (ERG), which is the electrical signal that is generated by the retina following a light stimulus. Of interest, the ERG was the first biopotential ever recorded (by Dewar in 1877) from a human subject. In the last 100 or so years, the recording technologies were tremendously improved, but the analysis of the ERG remained limited to the basic time domain measurement of its <em>amplitude</em> and <em>latency</em>.</p>

<p>When I started my PhD project, Dr. Lachapelle, who is the head of the McGill Visual Electrophysiology Clinic and Laboratory, asked me: “Is it possible to modernize the ERG analysis in order to bring it to the 21st century?” I told him “Yes!” and it became the goal of my PhD project! To do so, I have used some of the most up-to-date signal processing techniques (many of which were developed in the laboratory of my co-supervisor, <a href="https://scholar.google.ca/citations?user=pYbiAC8AAAAJ"><em>Dr. Jean-Marc Lina</em></a>) to analyze normal and pathological ERGs. My project thus addressed an important question: «Can advanced analytical approaches uncover additional useful information from ERG recordings?». Specifically, I undertook to complement the classical time domain analyzes of amplitude and latency of ERG responses with the investigation of the <em>oscillation</em> <em>frequencies</em> underlying different ERG components. Indeed, my findings showed that studying the time-frequency domain could significantly improve our basic understanding of the ERG and its clinical usefulness.</p>

<table>
  <tbody>
    <tr>
      <td><img src="/assets/images/mathieu.png" width="80%" /></td>
    </tr>
    <tr>
      <td><em>Mathieu working through his last datasets</em></td>
    </tr>
  </tbody>
</table>

<hr />

<p><strong>A: This sound really exciting! I have a question now about the specific types of ERG responses you analyzed. You used the discrete wavelet transform to measure the oscillation frequencies contributing to the a- and b-wave responses in the ERG. What do these two types of waves stand for?</strong></p>

<p><strong>M:</strong> In your retinas, your photoreceptors capture incoming photons (i.e. light) and convert this energy into electrical responses that subsequently activate other specialized retinal cells (e.g., bipolar cells, Müller cells, etc.), ultimately leading to the transmission of visual information to the brain via the optic nerve. Therefore, following a light stimulus, a sequence of bioelectrical retinal events can be measured non-invasively with an electrode located on the eye (e.g., the cornea) or close to it (e.g., the eye lid or temple). The signal thereby obtained represents the ERG. The a- and b-waves are simply the two main (i.e., biggest) waves of the ERG waveform (see Figure 1A). The a-wave is the first negative deflection (mostly reflecting the activity of the photoreceptors) and the b-wave is the second positive wave (which accounts mostly for the activity of bipolar and Müller cells).</p>

<table>
  <tbody>
    <tr>
      <td><img src="/assets/images/mathieu-data.png" width="80%" /></td>
    </tr>
    <tr>
      <td><em>Figure 1A</em></td>
    </tr>
  </tbody>
</table>

<hr />

<p><strong>A: I see. And as far as I’ve understood, you found that independent processes (“sub-components” of the b-wave) operate in different frequency bands of the ERG, right? What is the distinct contribution of each frequency band? Are these mechanisms elicited by different cell types?</strong></p>

<p><strong>M:</strong> We found that in normal individuals, the main components of the ERG b-wave oscillate in the 20Hz and 40Hz frequency bands. These frequency components were thus termed 20b and 40b, respectively. These two descriptors were differently modulated by the same light stimuli, suggesting that these two processes are independent. Moreover, 20b and 40b contribute differently to different types of retinal pathway anomaly: in the ON retinal pathway anomaly we found a marked reduction of the 20b, whereas the OFF retinal pathway anomaly revealed a severe reduction of the 40b. Because we know that patients affected with ON or OFF pathway anomaly have a conduction deficit that prevents their ON or OFF bipolar cells to be stimulated, these two frequency components might be linked to neuroanatomy (e.g., to the ON and OFF bipolar cells). To confirm this assumption, one would need to use pharmacological blockade of ON and OFF bipolar cells; but, irrespective of this confirmation, we showed that the discrete wavelet transform reveals reproducible, physiologically meaningful, and diagnostically relevant descriptors of the ERG over a wide range of signal amplitudes and morphologies.</p>

<hr />

<p><strong>A: Does that also mean that your results are potentially clinically applicable? In general, what is the clinical relevance of ERGs, and how could your findings change the status quo (i.e., regarding assessments of visual impairments)?</strong></p>

<p><strong>M:</strong> Currently, examination of the retina with the ophthalmoscope is one of the most widespread clinical exams. Furthermore, the normal function of any retinal cells can be changed by numerous pathological processes – and, as a result, these functional changes will alter the amplitude and/or timing of some ERG components (see figure 1B). Given that the ophthalmoscope does not always reveal signs of the suspected retinopathy on the retina, the ERG is often considered to be a more objective diagnostic tool. Unfortunately, amplitude and peak-time measurements of the ERG have some significant diagnostic limitations, which might explain the lower popularity of this clinical test. This is better illustrated in Figure 1B where four pathological ERGs, which are similarly reduced in terms of amplitude, present with strikingly different morphologies. Using the traditional analysis approach (e.g., amplitude of the b-wave), these ERG waveforms would all be classified in the «reduced amplitude» category and are thus all considered to be equivalent. Our latest study suggests that the use of the discrete wavelet transform will be useful to distinguish between the ERGs that previously were erroneously considered as equivalent using the traditional approach. Specifically, we could segregate ERGs recorded from patient affected with diseases that specifically alter the function of the ON and OFF cone pathway. Our findings suggest that the analysis of the ERG using the discrete wavelet transform is a valuable addition to the electrophysiologist’s armamentarium that has an immense potential for improving the quantification and interpretation of normal and pathological ERG responses.</p>

	  ]]></description>
	</item>

	<item>
	  <title>Interview with Emily Coffey</title>
	  <link>//2016/04/27/emily-coffey-ipn-interview/</link>
	  <author></author>
	  <pubDate>2016-04-27T15:01:00-04:00</pubDate>
	  <guid>//2016/04/27/emily-coffey-ipn-interview/</guid>
	  <description><![CDATA[
	     <blockquote>
  <p>Finally, we are starting to post interviews with our fellow graduate students in neuroscience at McGill! The first Q&amp;A session we held was with Emily Coffey, Ph.D. student in <a href="http://www.zlab.mcgill.ca/home.php" target="_blank">the lab of Dr. Robert Zatorre</a>. Earlier this year, Emily’s <a href="http://www.nature.com/ncomms/2016/160324/ncomms11070/full/ncomms11070.html" target="_blank">first-authored article based on her magnetoencephalography (MEG) investigation of the auditory frequency-following response</a> (FFR) was published in Nature Communications. In their study, Emily and her colleagues showed that the FFR, a neurophysiological response to complex periodic sounds previously considered to be of purely subcortical origins, may in fact partially reflect contributions from the human auditory cortex. In other words, the use of MEG (instead of the electroencephalography, typically used for studying auditory brainstem responses) allowed the authors to see that the auditory cortex works together with the brainstem during sound periodicity analysis. NeuroBlog editor <a href="https://blog.gsaneuro.com/editors/" target="_blank">Anastasia Glushko</a> spoke to Emily about this study, and we hope you’ll enjoy reading what came out of this conversation!</p>
</blockquote>

<hr />

<p><strong>A (Anastasia): To start from the basics, I wanted to ask you about the collaboration of the brainstem and the auditory cortex (AC) during sound periodicity analysis. You mention in the article that the connection between these two brain regions can potentially work in both directions. On the one hand, the signals from the brainstem can be transferred to the auditory cortex. But the auditory cortex might also influence the brainstem when fine-grained analysis of sound frequency is performed. What do you think could be the functionality of these two directions of the information flow (also taken into account that the AC seems to be activated later than the subcortical structures during sound frequency processing)?</strong></p>

<p><strong>E (Emily)</strong>: We know from anatomical work that there are a lot of descending projections in the auditory system (in fact more than ascending projections), but we don’t yet know too much about how lower and higher level areas work together to process sound. This is one of the things we are currently trying to understand about how the auditory system works, and what our new MEG technique will hopefully be useful for.</p>

<p>Interestingly, early signals can be influenced by many things like what you’re listening to, whether the sound is played forwards or backwards, or whether or not you know how to speak a tonal language, like Mandarin. Those observations suggest involvement of higher-level processes like selective attention, but as you point out, these these effects often cannot be an <em>elicited</em> response to sound because of the timescales involved - the signal cannot have made it up to the cortex and then back down to the brainstem again in that short an interval of time. This suggests that top-down processes have either acted on the lower parts of the system to change how they process incoming sound in a relatively permanent fashion (as in the case of long-term music or language experience), or they might be acting to filter sound in an online temporary fashion, like when you direct your attention to one person talking in a crowded restaurant.</p>

<p>The brain signals we’re working with can also be elicited when people are sleeping or awake but distracted doing something else. This is perhaps a good measure of the feed-forward part of the signal, since processes like selective attention are not engaged, but it still could of course have been influenced by long-term training.</p>

<p>Sound is a very complex and rich source of information, and the brain seems to have evolved “built-in” ways of separating and manipulating it, the ability to tune and change those mechanisms over long periods of time, and the capability of voluntarily modifying incoming signals according to task demands. How well the information is preserved as it comes in and is then filtered, enhanced and processed by higher-level systems in turn affects the quality of information that is sent on for purposes like language processing. We have a lot of work to do to figure out how all this takes place!</p>

<hr />

<p><strong>A: The AC activations in your study were right lateralized. This is in line with the often reported differences in the functionality of the left and right ACs. The right AC seems to be relatively specific for fine-grained sound frequency analysis while the left AC is “better” at temporal resolution. In the article, you mention the possibility that the computations on the brainstem level might contribute to the asymmetry in the AC. Does this mean that certain processes in the brainstem might have lead to the fact that left hemisphere is more tuned for language processing (which normally requires better temporal resolution than music perception)?</strong></p>

<p><strong>E</strong>: A lot of processing does go on at the level of the brainstem in the auditory system. Given the level of top-down connectivity, it wouldn’t surprise me if signal frequency specialization was already beginning in these lower areas. However, I don’t think we can say much about how this contributes to the cortical lateralization we see here in humans, from this technique. Although we can differentiate between signals from the left and right cortex and different levels of the brainstem, the pairs of brainstem nuclei are close together and deep, so they will be difficult to resolve using non-invasive methods.</p>

<hr />

<p><strong>A: It is exciting that the research on auditory brainstem responses has clear clinical potential. You write in the article that certain FFR parameters are reliable biomarkers of some clinical syndromes. What are the existing and/or potential applications of FFR measures in clinical practice? And how does the finding of the cortical component in the FFR contribute to fields where measures of auditory brainstem responses are used in clinical settings?</strong></p>

<p><strong>E</strong>: The frequencies observed using the FFR are important for how we perceive the pitch and timbre of sounds. Many problems that have language-related components show differences in the FFR, suggesting that there are abnormalities or deficiencies in how sound is processed. These include autism, language-related learning disorders, and deciphering speech in noisy conditions in older people and in children. The FFR could be used to identify sub-classes of populations that might benefit from a certain kind of treatment and to track improvements in the FFR and behavior over time. There is also evidence to suggest that music practice enhances the FFR and mitigates some of these problems, which would certainly make for an enjoyable treatment solution! The work on FFRs as biomarkers and as measures of treatments is still in development (<a href="http://brainvolts.northwestern.edu" target="_blank">Prof. Nina Kraus’s lab</a> at Northwestern University is doing some very interesting work on these topics).</p>

<p>The finding of a cortical component to the FFR is I think most relevant on the research side. I doubt we’ll be measuring people’s cortical MEG-FFR responses in the clinic because EEG is far cheaper and more practical! But I hope it will be useful to better understand what the EEG signal means, where it comes from, and which brain regions are at fault when poor auditory system function is contributing to clinical problems.</p>

	  ]]></description>
	</item>

